{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install trafilatura requests bs4 fitz pytesseract pillow surya-ocr faster-whisper openai-whisper datasketch\n",
    "\n",
    "# install ffmpg for Whisper to process your audio\n",
    "# On macOS (with Homebrew)\n",
    "# ! brew install ffmpeg\n",
    "# On Ubuntu/Debian:\n",
    "# ! sudo apt-get update -y\n",
    "# ! sudo apt-get install -y ffmpeg\n",
    "# ðŸ‘‰ On Windows (if using WSL or native):\n",
    "# You can download it from:\n",
    "# ðŸ”— https://ffmpeg.org/download.html\n",
    "# Or use a package manager like Chocolatey:\n",
    "# ! choco install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Pretraining Data Collection & Extraction - Hands-on Notebook\n",
    "\n",
    "## 1. Clean Web Page Text Using trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Install dependencies if not already installed\n",
    "import trafilatura\n",
    "print(trafilatura.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "print(requests.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: An arXiv paper abstract page\n",
    "url = \"https://arxiv.org/abs/2404.00001\"\n",
    "\n",
    "# Step 1: Fetch raw HTML\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# Step 2: Use Trafilatura to extract clean text\n",
    "downloaded_text = trafilatura.extract(html, include_comments=False, include_tables=False)\n",
    "\n",
    "# Step 3: Display the result\n",
    "print(\"ðŸ“„ Extracted Text Preview:\\n\")\n",
    "print(downloaded_text[:1000])  # Show first 1000 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "trafilatura.extract() pulls main article content while removing headers, menus, and boilerplate.\n",
    "\n",
    "This works great on academic websites like arXiv, blog posts, or news articles.\n",
    "\n",
    "No need to write custom HTML parsers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: OCR â€“ Convert Images to Text\n",
    "### Option A: Tesseract OCR (Offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might use the following install if the pytesseract is not installed\n",
    "# ! sudo apt-get update -y\n",
    "# ! sudo apt-get install -y tesseract-ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install: sudo apt install tesseract-ocr OR !pip install pytesseract Pillow\n",
    "import pytesseract\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess image (convert to grayscale)\n",
    "image = Image.open(\"C:\\\\Users\\\\ch939\\\\Downloads\\\\LLMBootCampCodes\\\\MyGPU.png\").convert(\"L\")  # grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pytesseract.image_to_string(image)\n",
    "\n",
    "print(\"ðŸ“„ Tesseract OCR Output (first 500 chars):\")\n",
    "print(text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Surya OCR (Fast PyTorch-based layout-aware tool)\n",
    "https://github.com/VikParuchuri/surya\n",
    "\n",
    "### Usage\n",
    "To perform OCR on an image, PDF, or a folder containing them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Good for: simple single-column text, PDFs converted to images\n",
    "* Struggles with layout, math, or low-res scans \n",
    "    * As you can see from the image: \"Download Models\" has not been extreact out correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch939\\anaconda3\\envs\\llmweek3env\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install git+https://github.com/datalab-to/surya.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:42:04) [MSC v.1943 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --extra-index-url https://pypi.org/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!surya_ocr --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! surya_ocr ./test_data/image/image.png --langs en --images --output_dir results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "**DATA_PATH** is the path to your image, PDF, or folder.\n",
    "\n",
    "**--langs** specifies the language(s) for OCR (e.g., en for English).\n",
    "\n",
    "**--images** saves images of the pages and detected text lines (optional).\n",
    "\n",
    "**--output_dir** specifies the directory to save results.â€‹\n",
    "\n",
    "This command will generate a results.json file containing the detected text and bounding boxes.â€‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Output Structure\n",
    "The **results.json** will have entries like:â€‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "  \"image\": [\n",
    "    {\n",
    "      \"text_lines\": [\n",
    "        {\n",
    "          \"polygon\": [\n",
    "            [\n",
    "              13,\n",
    "              48\n",
    "            ],\n",
    "            [\n",
    "              538,\n",
    "              51\n",
    "            ],\n",
    "            [\n",
    "              538,\n",
    "              87\n",
    "            ],\n",
    "            [\n",
    "              12,\n",
    "              84\n",
    "            ]\n",
    "          ],\n",
    "          \"confidence\": 0.9970703125,\n",
    "          \"text\": \"Llama 4: Leading intelligence.\",\n",
    "          \"bbox\": [\n",
    "            12,\n",
    "            48,\n",
    "            538,\n",
    "            87\n",
    "          ]\n",
    "        },\n",
    "        ...\n",
    "        {\n",
    "          \"polygon\": [\n",
    "            [\n",
    "              47,\n",
    "              364\n",
    "            ],\n",
    "            [\n",
    "              176,\n",
    "              364\n",
    "            ],\n",
    "            [\n",
    "              176,\n",
    "              378\n",
    "            ],\n",
    "            [\n",
    "              47,\n",
    "              378\n",
    "            ]\n",
    "          ],\n",
    "          \"confidence\": 0.9716796875,\n",
    "          \"text\": \"Download models\",\n",
    "          \"bbox\": [\n",
    "            47,\n",
    "            364,\n",
    "            176,\n",
    "            378\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"languages\": [\n",
    "        \"en\"\n",
    "      ],\n",
    "      \"image_bbox\": [\n",
    "        0,\n",
    "        0,\n",
    "        600,\n",
    "        471\n",
    "      ],\n",
    "      \"page\": 1\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### or in python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from surya.detection import DetectionPredictor\n",
    "from surya.recognition import RecognitionPredictor\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(\"./test_data/image/image.png\")  # Replace with your image path\n",
    "langs = [\"en\"]  # Specify the language(s)\n",
    "\n",
    "# Initialize predictors\n",
    "detection_predictor = DetectionPredictor()\n",
    "recognition_predictor = RecognitionPredictor()\n",
    "\n",
    "# Perform OCR\n",
    "predictions = recognition_predictor([image], [langs], detection_predictor)\n",
    "\n",
    "# Display results with polygon coordinates\n",
    "for page in predictions:\n",
    "    for line in page.text_lines:\n",
    "        print(f\"Text: {line.text}\")\n",
    "        print(f\"Confidence: {line.confidence}\")\n",
    "        print(f\"Polygon: {line.polygon}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Good for: structured layouts like academic papers\n",
    "* Fast inference and easy to integrate with PDF workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option C: OpenAI GPT-4o Vision OCR (Highly Accurate & Multicolumn)\n",
    "don't forget to add you `OPENAI_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def vision_extract(b64_image, prompt, api_key):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"temperature\": 0.0,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64_image}\"}}\n",
    "            ]}\n",
    "        ],\n",
    "        \"max_tokens\": 3000\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Load image and run GPT-4o OCR\n",
    "with open(\"test_data/image/image.png\", \"rb\") as f:\n",
    "    b64_img = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "# Use your actual API key here\n",
    "result = vision_extract(b64_img, \"Extract all the readable text from this document.\", api_key=\"YOUR_OPENAI_API_KEY\")\n",
    "print(result[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Good for: complex, multi-column documents and natural layout reasoning\n",
    "* Great fallback when you need accuracy over speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automatic Speech Recognition (ASR)\n",
    "### Option A: Whisper by OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! brew install ffmpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install: pip install openai-whisper\n",
    "import whisper\n",
    "\n",
    "# Load model\n",
    "model = whisper.load_model(\"base\")  # or \"small\", \"medium\", \"large\"\n",
    "\n",
    "# Transcribe audio\n",
    "result = model.transcribe(\"./test_data/audio/sample-1.mp3\")\n",
    "print(\"ðŸ“„ Whisper Transcription:\")\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Great for: balanced speed and accuracy\n",
    "* Supports many audio formats: mp3, wav, m4a, webm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Faster-Whisper (Fast & Lightweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install faster-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Load model with float16 for speed\n",
    "model = WhisperModel(\"base\", device=\"cpu\", compute_type=\"int8\")  # For CPUs\n",
    "\n",
    "# Transcribe\n",
    "segments, _ = model.transcribe(\"./test_data/audio/sample-1.mp3\")\n",
    "\n",
    "print(\"ðŸ“„ Faster-Whisper Transcription:\")\n",
    "for segment in segments:\n",
    "    print(f\"[{segment.start:.2f} - {segment.end:.2f}] {segment.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimized for GPU or even CPU \n",
    "* Useful when batch-processing long audio datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pretraining Data Cleaning Pipeline\n",
    "### Step 1: Remove duplicates using MinHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "def minhash_deduplication(texts, threshold=0.7):\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=128)\n",
    "    unique_texts = []\n",
    "    for i, doc in enumerate(texts):\n",
    "        m = MinHash(num_perm=128)\n",
    "        for word in set(doc.split()):\n",
    "            m.update(word.encode('utf8'))\n",
    "        if not lsh.query(m):\n",
    "            lsh.insert(f\"doc{i}\", m)\n",
    "            unique_texts.append(doc)\n",
    "    return unique_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Filter for language and strip HTML noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html_and_filter_lang(texts, lang='en'):\n",
    "    filtered = []\n",
    "    for txt in texts:\n",
    "        txt = BeautifulSoup(txt, 'html.parser').get_text()\n",
    "        try:\n",
    "            if detect(txt.strip()) == lang:\n",
    "                filtered.append(txt.strip())\n",
    "        except:\n",
    "            continue\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Strip PII using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def strip_pii(text):\n",
    "    text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '[EMAIL]', text)\n",
    "    text = re.sub(r'\\b\\d{12,19}\\b', '[CREDIT_CARD]', text)\n",
    "    text = re.sub(r'\\b(?:\\d{3}-){2}\\d{4}\\b', '[PHONE]', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Remove repetitive n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def remove_repetitive_ngrams(text, n=3, threshold=3):\n",
    "    words = text.split()\n",
    "    ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "\n",
    "    counts = Counter(ngrams)\n",
    "    repetitive = [ngram for ngram, count in counts.items() if count >= threshold]\n",
    "\n",
    "    for phrase in repetitive:\n",
    "        # regex-safe version of the phrase\n",
    "        escaped_phrase = re.escape(phrase)\n",
    "        # match the phrase repeated 2+ times with optional whitespace\n",
    "        text = re.sub(rf'(?:{escaped_phrase}\\s*){{{threshold},}}', phrase + ' ', text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s{2,}', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: prepare for the text data\n",
    "load the Fake_pretraining_Texts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fake_texts = pd.read_csv(\"test_data/data/Fake_Pretraining_Texts.csv\")\n",
    "raw_dataset = fake_texts[\"Raw Text\"]\n",
    "print(raw_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Apply the Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove HTML + Language Filter\n",
    "step1 = clean_html_and_filter_lang(raw_dataset)\n",
    "display(step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Deduplicate Paragraphs\n",
    "step2 = minhash_deduplication(step1)\n",
    "display(step2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Strip PII\n",
    "step3 = [strip_pii(t) for t in step2]\n",
    "display(step3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Remove Repetitive N-grams\n",
    "cleaned_data = [remove_repetitive_ngrams(t) for t in step3]\n",
    "display(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done!\n",
    "print(\"âœ… Cleaned dataset sample:\")\n",
    "for idx, text in enumerate(cleaned_data):\n",
    "    print(f\"--- Article {idx + 1} ---\")\n",
    "    print(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llmweek3env)",
   "language": "python",
   "name": "llmweek3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
